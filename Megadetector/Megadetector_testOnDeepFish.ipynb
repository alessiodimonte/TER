{"cells":[{"cell_type":"markdown","metadata":{"id":"s-Nso4g7VXxy"},"source":["<a href=\"https://colab.research.google.com/github/microsoft/CameraTraps/blob/master/detection/megadetector_colab.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n","</a>\n","\n","Link in case the above badge doesn't redirect you correctly: [Open in Colab](https://colab.research.google.com/github/microsoft/CameraTraps/blob/master/detection/megadetector_colab.ipynb)\n","\n","This notebook replaces a previous example by [@louis030195](https://github.com/louis030195). Improvements: updated environment setup, MegaDetector model version and support for mounting Google Drive folders so you can process your own images here."]},{"cell_type":"markdown","metadata":{"id":"vUXNQZtwEYiQ"},"source":["# Running MegaDetector on camera trap images using Google Colab\n","Put together by Alistair Stewart, Alice Springs, May 2020.\n","@alsnothome\n","\n","For reference please read the [MegaDetector guide on GitHub](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md) and check there for updates. Here we have roughly followed the steps for running under Linux.\n","\n","This notebook is designed to load camera trap image files already uploaded onto Google Drive. If you don't have images already loaded onto Google Drive or just want to see a demo of MegaDetector in action, we also provide code to download some sample images.\n","\n","The steps walk through copying of all of the required model and helper files to the Colab runtime and installing all the required packages. You can then connect to your Google Drive folder and process all of the images in a folder using the MegaDetector saved model. The output is saved in a JSON file - a text based database file whose format is described in this [section](https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#batch-processing-api-output-format) in the batch API user guide. The detections (as bounding boxes) can then be rendered on your images.\n","\n","The Google Colab instance will only stay open for a maximum 10-12 hrs and after that it will close and any unsaved data will be lost. We recommend saving the JSON output and annotated images into your Google Drive folder for persistent storage."]},{"cell_type":"markdown","metadata":{"id":"9aUlxnm7cnWy"},"source":["## Set up the Colab instance to run on GPU processing\n","\n","\n","Navigate to Edit→Notebook Settings and select \"GPU\" from the Hardware Accelerator drop-down "]},{"cell_type":"markdown","metadata":{"id":"LUyqKSAWRGNw"},"source":["## Copy the model, install dependencies, set PYTHONPATH\n","\n","Note: from here on you'll start seeing a mix of code. Most are Linux system commands, rather than Python. The system commands are prefixed by a shebang `!`, which tells this notebook to execute them on the command line."]},{"cell_type":"markdown","metadata":{"id":"ddPlAKHFTn3m"},"source":["### Install TensorFlow v1\n","\n","TensorFlow is already installed in Colab, but our scripts are not yet compatible with the newer version of TensorFlow. \n","\n","Please follow the next three steps in sequence and do not skip any steps :) If you were not able to follow these, you can reset the runtime by going to \"Runtime\" in the top menu and \"Factory reset runtime\".\n","\n","\n","1. Uninstall the existing version of TensorFlow (this doesn't affect your other Colabs, don't worry)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_doqNfDfsi66","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978057198,"user_tz":-60,"elapsed":24779,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"ff6a653f-70bd-4873-b409-629dda011975"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.7.0\n","Uninstalling tensorflow-2.7.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.7/dist-packages/tensorflow-2.7.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.7.0\n"]}],"source":["pip uninstall tensorflow"]},{"cell_type":"markdown","metadata":{"id":"ILf1Luogsi68"},"source":["2. Install the older TensorFlow version using `pip`, with GPU processing by specifying `-gpu` and version number `1.13.1`. We also install the other required Python packages that are not already in Colab - `humanfriendly` and `jsonpickle`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMEkgpy6T0pr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978104146,"user_tz":-60,"elapsed":44494,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"14adf655-8265-42ab-96df-9302f2298a7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu==1.13.1\n","  Downloading tensorflow_gpu-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (345.0 MB)\n","\u001b[K     |████████████████████████████████| 345.0 MB 4.4 kB/s \n","\u001b[?25hCollecting humanfriendly\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 1.7 MB/s \n","\u001b[?25hCollecting jsonpickle\n","  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.42.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.12.0)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 42.0 MB/s \n","\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n","\u001b[K     |████████████████████████████████| 367 kB 28.3 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.37.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.4.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (1.19.5)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (3.17.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (3.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.10.0.2)\n","Collecting mock>=2.0.0\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (1.5.2)\n","Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow-gpu, jsonpickle, humanfriendly\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","Successfully installed humanfriendly-10.0 jsonpickle-2.0.0 keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"]}],"source":["pip install tensorflow-gpu==1.13.1 humanfriendly jsonpickle"]},{"cell_type":"markdown","metadata":{"id":"F3VHcB1Qsi69"},"source":["3. Importantly, you now need to **re-start the runtime** of this Colab for it to start using the older version TensorFlow that we just installed.\n","\n","Click on the \"Runtime\" option on the top menu, then \"Restart runtime\". After that, you can proceed with the rest of this notebook.\n","\n","Let's check that we have the right version of TensorFlow (1.13.1):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGp4Kf-esi69","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978116181,"user_tz":-60,"elapsed":2428,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"a64dcf3e-e66c-4a4e-e4a7-11ffb2e76b25"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"]},{"output_type":"stream","name":"stdout","text":["1.13.1\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"hXn_-PZqTWB4"},"source":["### Download the MegaDetector model file\n","\n","Currently, v4.1 is avaialble by direct download. The link can be found in the GitHub MegaDetector readme: MegaDetector v4.1, 2020.04.27 frozen model (.pb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5uwmpmaTZMX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978131173,"user_tz":-60,"elapsed":6145,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"7d0fa488-e95f-4c16-90fa-2e0144826bd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-08 15:42:04--  https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb\n","Resolving lilablobssc.blob.core.windows.net (lilablobssc.blob.core.windows.net)... 52.239.159.84\n","Connecting to lilablobssc.blob.core.windows.net (lilablobssc.blob.core.windows.net)|52.239.159.84|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 245590501 (234M) [application/octet-stream]\n","Saving to: ‘/content/megadetector_v4_1_0.pb’\n","\n","/content/megadetect 100%[===================>] 234.21M  44.7MB/s    in 5.7s    \n","\n","2021-12-08 15:42:10 (41.4 MB/s) - ‘/content/megadetector_v4_1_0.pb’ saved [245590501/245590501]\n","\n"]}],"source":["!wget -O /content/megadetector_v4_1_0.pb https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb"]},{"cell_type":"markdown","metadata":{"id":"nmJ6lQX8S4im"},"source":["### Clone the two required Microsoft git repos\n","This will copy the latest version of the Microsoft AI for Earth \"utilities\" and \"Camera Traps\" repositories from GitHub. These make data handling and running the model easy. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qhltAaRSe1W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978150549,"user_tz":-60,"elapsed":7580,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"2fb0dafd-4e13-40f6-8f4a-50f030e812aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CameraTraps'...\n","remote: Enumerating objects: 12276, done.\u001b[K\n","remote: Counting objects: 100% (1264/1264), done.\u001b[K\n","remote: Compressing objects: 100% (702/702), done.\u001b[K\n","remote: Total 12276 (delta 739), reused 1023 (delta 558), pack-reused 11012\u001b[K\n","Receiving objects: 100% (12276/12276), 122.05 MiB | 27.74 MiB/s, done.\n","Resolving deltas: 100% (7219/7219), done.\n","Cloning into 'ai4eutils'...\n","remote: Enumerating objects: 728, done.\u001b[K\n","remote: Counting objects: 100% (309/309), done.\u001b[K\n","remote: Compressing objects: 100% (224/224), done.\u001b[K\n","remote: Total 728 (delta 187), reused 171 (delta 84), pack-reused 419\u001b[K\n","Receiving objects: 100% (728/728), 2.63 MiB | 13.45 MiB/s, done.\n","Resolving deltas: 100% (418/418), done.\n"]}],"source":["!git clone https://github.com/microsoft/CameraTraps\n","!git clone https://github.com/microsoft/ai4eutils"]},{"cell_type":"markdown","metadata":{"id":"XQTdfBPZiXiV"},"source":["We'll also copy the Python scripts that run the model and produce visualization of results to the working directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4Ns5PjeiTro"},"outputs":[],"source":["!cp /content/CameraTraps/detection/run_tf_detector_batch.py .\n","!cp /content/CameraTraps/visualization/visualize_detector_output.py ."]},{"cell_type":"markdown","metadata":{"id":"2pzfM5Y-iby1"},"source":["### Set `PYTHONPATH` to include `CameraTraps` and `ai4eutils`\n","\n","Add cloned git folders to the `PYTHONPATH` environment variable so that we can import their modules from any working directory.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8vanlgAOlEj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978172002,"user_tz":-60,"elapsed":255,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"deca2bde-1142-4566-9306-367ea045294d"},"outputs":[{"output_type":"stream","name":"stdout","text":["PYTHONPATH: /env/python:/content/ai4eutils:/content/CameraTraps\n"]}],"source":["import os\n","os.environ['PYTHONPATH'] += \":/content/ai4eutils\"\n","os.environ['PYTHONPATH'] += \":/content/CameraTraps\"\n","\n","!echo \"PYTHONPATH: $PYTHONPATH\""]},{"cell_type":"markdown","metadata":{"id":"JyjEgkCsOsak"},"source":["## Mount Google Drive in Colab\n","You can mount your Google Drive if you have sample images there to try MegaDetector on or want to save the results to your Google Drive.\n","\n","Once you run the cell below, it will show a URL and a text box.\n","\n","Visit that URL to choose the Google account where the images you want to process live. After you authenticate, an authorization code will be shown. Copy the authorization code to the text box here. \n","\n","Your Google Drive folders will then be mounted under `/content/drive` and can be viewed and navigated in the Files pane.\n","\n","The method is described under this Colab code snippet: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA. Never give out your account username and password. Read this Colab code snippet to understand how this connection is made and authenticated. There are other ways to connect your Google Drive or upload your data if you do not find this method suitable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYsrTTR7eF0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978434745,"user_tz":-60,"elapsed":16473,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"bc4cd475-5c9e-49f2-a04e-57a79f48f937"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"UEX6wZaOsi7E"},"source":["## Optional: Download Sample Images\n","\n","We install Microsoft Azure's [`azcopy` utility](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) which we then use to download camera trap images from the [Snapshot Serengeti](http://lila.science/datasets/snapshot-serengeti) dataset hosted on [lila.science](http://lila.science)."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"xhd8bgPdsi7E"},"outputs":[],"source":["%%bash\n","# download azcopy\n","wget -O azcopy_linux.tar.gz https://aka.ms/downloadazcopy-v10-linux\n","tar -xvzf azcopy_linux.tar.gz --wildcards */azcopy --strip 1\n","rm azcopy_linux.tar.gz\n","chmod u+x azcopy\n","\n","# copy Snapshot Serengeti images to a local directory\n","DATASET_URL=\"https://lilablobssc.blob.core.windows.net/snapshotserengeti-unzipped/\"\n","SAMPLE_DIR=\"S1/D05/D05_R4\"\n","SAS_TOKEN=\"?st=2020-01-01T00%3A00%3A00Z&se=2034-01-01T00%3A00%3A00Z&sp=rl&sv=2019-07-07&sr=c&sig=/DGPd%2B9WGFt6HgkemDFpo2n0M1htEXvTq9WoHlaH7L4%3D\"\n","LOCAL_DIR=\"/content/snapshotserengeti\"\n","\n","./azcopy cp \"${DATASET_URL}${SAMPLE_DIR}${SAS_TOKEN}\" \"${LOCAL_DIR}\" --recursive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCla9Yemsi7E"},"outputs":[],"source":["# this should show 48 images downloaded\n","!ls /content/snapshotserengeti/ -R"]},{"cell_type":"markdown","metadata":{"id":"_a02h0yZsi7F"},"source":["## Optional: Download Sample Images\n","\n","We install Microsoft Azure's [`azcopy` utility](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) which we then use to download camera trap images from the [Snapshot Serengeti](http://lila.science/datasets/snapshot-serengeti) dataset hosted on [lila.science](http://lila.science)."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"mQEUGYemsi7F"},"outputs":[],"source":["%%bash\n","# download azcopy\n","wget -O azcopy_linux.tar.gz https://aka.ms/downloadazcopy-v10-linux\n","tar -xvzf azcopy_linux.tar.gz --wildcards */azcopy --strip 1\n","rm azcopy_linux.tar.gz\n","chmod u+x azcopy\n","\n","# copy Snapshot Serengeti images to a local directory\n","DATASET_URL=\"https://lilablobssc.blob.core.windows.net/snapshotserengeti-unzipped/\"\n","SAMPLE_DIR=\"S1/D05/D05_R4\"\n","SAS_TOKEN=\"?st=2020-01-01T00%3A00%3A00Z&se=2034-01-01T00%3A00%3A00Z&sp=rl&sv=2019-07-07&sr=c&sig=/DGPd%2B9WGFt6HgkemDFpo2n0M1htEXvTq9WoHlaH7L4%3D\"\n","LOCAL_DIR=\"/content/snapshotserengeti\"\n","\n","./azcopy cp \"${DATASET_URL}${SAMPLE_DIR}${SAS_TOKEN}\" \"${LOCAL_DIR}\" --recursive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1Pn5tAHsi7G"},"outputs":[],"source":["# this should show 48 images downloaded\n","!ls /content/snapshotserengeti/ -R"]},{"cell_type":"markdown","metadata":{"id":"Lkugt7r3uUEr"},"source":["## MegaDetector batch processing\n","\n","This step executes the Python script `run_tf_detector_batch.py` that we copied from the CameraTraps repo. It has three mandatory arguments and one optional:\n","\n","1.   path to the MegaDetector saved model file.\n","2.   a folder containing images. If your images were already on Google Drive, replace `[Image_Folder]` with your folder name from Google Drive. If you are using the sample images from Snapshot Serengeti, change `images_dir` to `'/content/snapshotserengeti'`.\n","3.   the output JSON file location and name - replace `[Output_Folder]` with your folder name and `[output_file_name.json]` with your file name.\n","4.   option `--recursive` goes through all subfolders to find and process all images within.\n","\n","You will need to change the image folder path and output file path, depending on your situation.\n","\n","In our experience the Colab system will take ~30 seconds to intialize and load the saved MegaDetector model. It will then iterate through all of the images in the folder specified. Processing initially takes a few seconds per image and usually settles to ~1 sec per image. That is ~60 images per minute or ~3600 images per hour. Limit the number of images in your folder so that all of the processing can be completed before the Colab session ends.\n","\n","If you see the error \"AssertionError: output_file specified needs to end with .json\" then you haven't update the output folder and file name in the line of code below properly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSIH-k0nfi73"},"outputs":[],"source":["images_dir = '/content/drive/My Drive/TER_images'\n","\n","# choose a location for the output JSON file\n","output_file_path = '/content/drive/My Drive/TER_results/output.json'"]},{"cell_type":"markdown","metadata":{"id":"Bsvuux-yhpLw"},"source":["Here we pass the Python variable value `output_file_path` you specified above to the bash commands below using `$` (double quoting as there are spaces in this path), to run the script. This is so that we can refer to the output file path later for visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AOKfviGuTNg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978821284,"user_tz":-60,"elapsed":56805,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"c8adcf71-9508-4de1-da2e-8ffa03d19aea"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 1.13.1\n","2021-12-08 15:52:46.452483: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-12-08 15:52:46.464987: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-12-08 15:52:46.465279: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556f8f069e40 executing computations on platform Host. Devices:\n","2021-12-08 15:52:46.465319: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n","2021-12-08 15:52:46.659203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 15:52:46.660024: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556f8f069ce0 executing computations on platform CUDA. Devices:\n","2021-12-08 15:52:46.660061: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n","2021-12-08 15:52:46.661359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n","name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n","pciBusID: 0000:00:04.0\n","totalMemory: 11.17GiB freeMemory: 11.12GiB\n","2021-12-08 15:52:46.661397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n","2021-12-08 15:52:46.662616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 15:52:46.662658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n","2021-12-08 15:52:46.662675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n","2021-12-08 15:52:46.662751: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-12-08 15:52:46.662808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","Is GPU available? tf.test.is_gpu_available: True\n","TensorFlow version: 1.13.1\n","2021-12-08 15:52:46.664217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n","2021-12-08 15:52:46.664275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 15:52:46.664303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n","2021-12-08 15:52:46.664323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n","2021-12-08 15:52:46.664397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","tf.test.is_gpu_available: True\n","11 image files found in the input directory\n","TFDetector: Loading graph...\n","TFDetector: Detection graph loaded.\n","2021-12-08 15:52:52.853154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n","2021-12-08 15:52:52.853266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 15:52:52.853298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n","2021-12-08 15:52:52.853318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n","2021-12-08 15:52:52.853434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","Loaded model in 6.02 seconds\n","  0% 0/11 [00:00<?, ?it/s]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000008.jpg\n","2021-12-08 15:53:02.455388: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0\n","2021-12-08 15:53:09.652946: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/map/while/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/map/while/ResizeToRange/stack_2) axis attribute is out of bounds: 0\n","2021-12-08 15:53:11.285426: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n","  9% 1/11 [00:27<04:36, 27.63s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000009.jpg\n"," 18% 2/11 [00:29<01:53, 12.57s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000002.jpg\n"," 27% 3/11 [00:31<01:01,  7.70s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000003.jpg\n"," 36% 4/11 [00:33<00:38,  5.54s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000001.jpg\n"," 45% 5/11 [00:35<00:25,  4.22s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000005.jpg\n"," 55% 6/11 [00:37<00:17,  3.45s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000004.jpg\n"," 64% 7/11 [00:39<00:11,  2.96s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000007.jpg\n"," 73% 8/11 [00:41<00:07,  2.62s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000006.jpg\n"," 82% 9/11 [00:43<00:04,  2.39s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000010.jpg\n"," 91% 10/11 [00:45<00:02,  2.25s/it]Processing image /content/drive/My Drive/TER_images/7117_Caranx_sexfasciatus_juvenile_f000000.jpg\n","100% 11/11 [00:47<00:00,  4.29s/it]\n","Finished inference in 53.36 seconds\n","Output file saved at /content/drive/My Drive/TER_results/output.json\n","Done!\n"]}],"source":["!python run_tf_detector_batch.py megadetector_v4_1_0.pb \"$images_dir\" \"$output_file_path\" --recursive"]},{"cell_type":"markdown","metadata":{"id":"-tHu5WUGDpcd"},"source":["## Visualize batch processing script outputs\n","\n","Here we use the `visualize_detector_output.py` in the `visualization` folder of the Camera Traps repo to see the output of the MegaDetector visualized on our images. It will save images annotated with the results (original images will *not* be modified) to the `[Visualization_Folder]` you specify here.\n","\n","The scripts take in a number of optional parameters to control output image size and how many are sampled (if you've processed a lot of images but only want to visualize the results on a few) - take a look at the `main()` function in the script to see what other parameters are available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iv6ph0l1obhr"},"outputs":[],"source":["visualization_dir = '/content/TER_visualization'  # pick a location for annotated images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"en3TbCftkWDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638978962084,"user_tz":-60,"elapsed":1974,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"3905eed8-668b-4f1a-f751-f09301db55b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["detection_categories provided\n","Detector output file contains 11 entries.\n","Starting to annotate the images...\n","100% 11/11 [00:00<00:00, 12.74it/s]\n","Rendered detection results on 11 images, saved to /content/TER_visualization.\n"]}],"source":["!python visualize_detector_output.py \"$output_file_path\" \"$visualization_dir\" --confidence 0.3 --images_dir \"$images_dir\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0AYUcBlm9BN"},"outputs":[],"source":["import os\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AglNEK0goyjA","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"152daP0OurrNWy6qipwlALjSHnLUVb9Oq"},"executionInfo":{"status":"ok","timestamp":1638978965002,"user_tz":-60,"elapsed":2926,"user":{"displayName":"Giuseppe Alessio Dimonte","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi92bMdm2tiLn95CSehYeZm0HAtS4s0HMKc2rcPg=s64","userId":"15660447563579558201"}},"outputId":"734c563f-ad9b-433b-c69f-77fb7f238a50"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for viz_file_name in os.listdir(visualization_dir):\n","  print(viz_file_name)\n","  im = Image.open(os.path.join(visualization_dir, viz_file_name))\n","  display(im)  # display() is an iPython method that comes with the notebook"]},{"cell_type":"markdown","metadata":{"id":"YElCsmZBsi7N"},"source":["## Next steps\n","\n","Now that you have tried applying the MegaDetector on your own images and assessed its effectiveness, here are some pointers to help you take advantage of the MegaDetector to label your survey images more quickly.\n","\n","\n","### Ways to use the output JSON in my workflow \n","\n","#### 1. Timelapse\n","\n","[Timelapse](http://saul.cpsc.ucalgary.ca/timelapse/pmwiki.php?n=Main.HomePage) is an open-source tool for annotating camera trap images. We have worked with the Timelapse developer to integrate the output of our API into Timelapse, so a user can:\n","\n","- Select or sort images based on whether they contain animal or people or vehicles.\n","- View bounding boxes during additional manual annotation steps (which may speed up review...)\n","\n","See our [Integration with Timelapse](https://github.com/microsoft/CameraTraps/blob/master/api/batch_processing/integration/timelapse.md) page for more information.\n","\n","![Screenshot showing the Timelapse application with MegaDetector output, shown as a bounding box around the detected animal](https://github.com/microsoft/CameraTraps/blob/master/api/batch_processing/integration/images/tl_boxes.jpg?raw=1)\n","\n","\n","**Subsetting the output JSON into more manageable JSONs to use with Timelapse**\n","\n","People using Timelapse often split up the workload and have multiple Timelapse databases per study. We wrote a simple Windows desktop app ([Camera Trap Batch API Output Manager App](https://github.com/microsoft/CameraTraps/blob/master/api/batch_processing/postprocessing/CameraTrapJsonManagerApp.md)) for splitting up the output JSON.\n","\n","\n","#### 2. Separating images into folders that contain animals/people/vehicles/nothing\n","\n","Some of our collaborators do their downstream labeling without Timelapse, by moving the images to separate folders containing animals/people/vehicles/nothing according to MegaDetector output. You can use the script [separate_detections_into_folders.py](https://github.com/microsoft/CameraTraps/blob/master/api/batch_processing/postprocessing/separate_detections_into_folders.py) to do that if you have some familiarity with running Python scripts. This script has only one external package (\"tqdm\") to install - you can also follow our [installation instructions](https://github.com/microsoft/CameraTraps#initial-setup) to run the script inside a conda virtual environment. \n","\n","\n","### Help me process millions and millions of images\n","\n","We offer a [service](https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#detector-batch-processing-api-user-guide) for processing a large quantity of camera trap images using the MegaDetector. We typically call this service on behalf of ecologist users once their images are uploaded. Please see our [collaborations](https://github.com/microsoft/CameraTraps/blob/master/collaborations.md) page for more information and get in touch with us."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4n9SJeHgsi7O"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Megadetector_Colab_test.ipynb","provenance":[{"file_id":"https://github.com/microsoft/CameraTraps/blob/master/detection/megadetector_colab.ipynb","timestamp":1638977952924}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}